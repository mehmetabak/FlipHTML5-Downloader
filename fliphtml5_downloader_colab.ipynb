{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FlipHTML5 Downloader - Google Colab Version\n",
        "\n",
        "This notebook allows you to download pages from a FlipHTML5 book and convert them to a PDF. It uses the Python script from the GitHub repository.\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. **Upload the Necessary Files:**\n",
        "   - Ensure you upload the required files from your Google Drive to the Colab environment or directly provide the URLs for downloading.\n",
        "   \n",
        "2. **Install Required Libraries:**\n",
        "   - The necessary Python libraries will be installed automatically.\n",
        "\n",
        "3. **Run the Code Cells:**\n",
        "   - Follow the prompts in each cell to input your book ID, start and end pages, and other parameters.\n",
        "\n",
        "4. **Download the Final PDF:**\n",
        "   - The final PDF will be saved to your Colab environment and can be downloaded."
      ],
      "metadata": {
        "id": "xLps8NsuaK1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installation\n"
      ],
      "metadata": {
        "id": "Mn25PrUKdGuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests fpdf pillow tqdm PyPDF2"
      ],
      "metadata": {
        "id": "OO3g-VBTc6Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Code"
      ],
      "metadata": {
        "id": "B5PbvfjIdOdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import threading\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from PyPDF2 import PdfMerger\n",
        "from fpdf import FPDF\n",
        "\n",
        "# Introduction message\n",
        "print(\"FlipHTML5 Downloader - Enhanced Version/arasTiR\")\n",
        "\n",
        "# User inputs\n",
        "bookID = input(\"Enter Book ID (e.g., 'ousy/stby'): \")\n",
        "start = input(\"Enter the start page number (leave empty for default: 1): \")\n",
        "end = input(\"Enter the end page number (leave empty for default: last page): \")\n",
        "folderName = input(\"Enter folder name for saving (leave empty to use Book ID): \") or bookID.replace(\"/\", \"-\")\n",
        "pdfName = input(\"Enter PDF filename (leave empty for default): \") or f\"{folderName}.pdf\"\n",
        "skipExisting = input(\"Skip existing files? (y/n): \").lower() == 'y'\n",
        "\n",
        "# Set default values if start or end are empty\n",
        "start = int(start) if start else 1\n",
        "end = int(end) if end else None  # 'None' will be set to the last page later\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs(folderName, exist_ok=True)\n",
        "\n",
        "# User-agent list to randomize requests\n",
        "useragents = [\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.17017',\n",
        "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.1 Safari/605.1.15'\n",
        "]\n",
        "\n",
        "# Fetch configuration from a remote URL\n",
        "def fetch_config():\n",
        "    config_url = f\"https://online.fliphtml5.com/{bookID}/javascript/config.js?1\"\n",
        "    headers = {'User-Agent': random.choice(useragents)}\n",
        "    try:\n",
        "        r = requests.get(config_url, headers=headers, timeout=50)\n",
        "        r.raise_for_status()\n",
        "        # Extract JSON part from JavaScript content\n",
        "        json_str = re.search(r'var htmlConfig = ({.*?});', r.text, re.DOTALL)\n",
        "        if json_str:\n",
        "            config_data = json.loads(json_str.group(1))  # Parse JSON\n",
        "            return config_data\n",
        "        else:\n",
        "            print(\"[-] JSON format not found in config content.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"[-] Error fetching config: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Function to clean taskID\n",
        "def clean_taskID(taskID):\n",
        "    if taskID.startswith('./files/large/'):\n",
        "        taskID = taskID[len('./files/large/'):]  # Remove './files/large/' prefix\n",
        "    taskID = re.sub(r'\\.webp$|\\.jpg$', '', taskID)  # Remove '.webp' or '.jpg' extension if present\n",
        "    return taskID\n",
        "\n",
        "# Function to download a single image\n",
        "def download_image(taskID):\n",
        "    taskID = clean_taskID(taskID)  # Clean the taskID\n",
        "    for ext in ['webp', 'jpg']:  # Try webp first, then jpg\n",
        "        filepath = f\"{folderName}/{taskID}.{ext}\"\n",
        "        URL = f\"https://online.fliphtml5.com/{bookID}/files/large/{taskID}.{ext}\"\n",
        "        headers = {'User-Agent': random.choice(useragents)}\n",
        "\n",
        "        try:\n",
        "            r = requests.get(URL, headers=headers, timeout=10)\n",
        "            if r.status_code == 200:\n",
        "                img = Image.open(BytesIO(r.content))\n",
        "\n",
        "                # Convert webp to jpg if needed\n",
        "                if ext == 'webp':\n",
        "                    jpg_filepath = f\"{folderName}/{taskID}.jpg\"\n",
        "                    img.convert(\"RGB\").save(jpg_filepath, \"JPEG\")\n",
        "                    print(f\"[+] Page {taskID} downloaded as {ext} and converted to .jpg\")\n",
        "                else:\n",
        "                    img.save(filepath)\n",
        "                    print(f\"[+] Page {taskID} downloaded as {ext}\")\n",
        "                return\n",
        "            else:\n",
        "                print(f\"[-] Page {taskID} failed to download ({ext}, HTTP {r.status_code})\")\n",
        "        except Exception as e:\n",
        "            print(f\"[-] Page {taskID} failed to download: {str(e)}\")\n",
        "\n",
        "# Download images in range with a progress bar and optional threading for optimization\n",
        "def download_images_concurrently(start, end, max_threads=5):\n",
        "    config = fetch_config()\n",
        "    if not config:\n",
        "        print(\"[-] Configuration fetch failed. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Extract page IDs from the configuration\n",
        "    pages = [page['n'][0] for page in config.get('fliphtml5_pages', [])]\n",
        "\n",
        "    # Ensure pages are in the correct order and total count\n",
        "    total_pages = len(pages)\n",
        "\n",
        "    # Ensure the start and end are within the correct range\n",
        "    if start < 1 or (end is not None and end > total_pages) or start > (end if end is not None else total_pages):\n",
        "        print(\"[-] Invalid page range specified.\")\n",
        "        return\n",
        "\n",
        "    # Set end to the last page if it was not specified\n",
        "    end = end if end is not None else total_pages\n",
        "\n",
        "    # Get the correct page IDs based on user input\n",
        "    filtered_pages = pages[start-1:end]\n",
        "\n",
        "    # Save page order to a file for accurate sorting later\n",
        "    with open(f\"{folderName}/page_order.txt\", \"w\") as f:\n",
        "        for page in filtered_pages:\n",
        "            f.write(f\"{page}\\n\")\n",
        "\n",
        "    threads = []\n",
        "    with tqdm(total=len(filtered_pages)) as pbar:\n",
        "        def worker(taskID):\n",
        "            filepath_webp = f\"{folderName}/{taskID}.webp\"\n",
        "            filepath_jpg = f\"{folderName}/{taskID}.jpg\"\n",
        "\n",
        "            if skipExisting and (os.path.exists(filepath_webp) or os.path.exists(filepath_jpg)):\n",
        "                print(f\"[ ] Page {taskID} already exists, skipping.\")\n",
        "            else:\n",
        "                download_image(taskID)\n",
        "            pbar.update(1)\n",
        "\n",
        "        for taskID in filtered_pages:\n",
        "            while len(threads) >= max_threads:\n",
        "                for thread in threads:\n",
        "                    if not thread.is_alive():\n",
        "                        threads.remove(thread)\n",
        "\n",
        "            t = threading.Thread(target=worker, args=(taskID,))\n",
        "            t.start()\n",
        "            threads.append(t)\n",
        "\n",
        "        for thread in threads:\n",
        "            thread.join()\n",
        "\n",
        "# Function to convert images to PDF\n",
        "def images_to_pdf(folder, pdf_filename=\"output.pdf\"):\n",
        "    pdf_files = []\n",
        "    image_list = []\n",
        "\n",
        "    # Extract pages from folder and sort by their original order\n",
        "    with open(f\"{folder}/page_order.txt\") as f:\n",
        "        page_order = [line.strip() for line in f]\n",
        "\n",
        "    for taskID in page_order:\n",
        "        image_path = os.path.join(folder, f\"{clean_taskID(taskID)}.jpg\")\n",
        "        if os.path.exists(image_path):\n",
        "            image_list.append(image_path)\n",
        "\n",
        "    # Generate PDF files in chunks\n",
        "    chunk_size = 50\n",
        "    num_chunks = (len(image_list) // chunk_size) + 1\n",
        "    for i in range(num_chunks):\n",
        "        chunk_filename = f\"{folder}/chunk_{i+1}.pdf\"\n",
        "        pdf_files.append(chunk_filename)\n",
        "\n",
        "        pdf = FPDF()\n",
        "        start_index = i * chunk_size\n",
        "        end_index = min((i + 1) * chunk_size, len(image_list))\n",
        "        for image_path in image_list[start_index:end_index]:\n",
        "            pdf.add_page()\n",
        "            pdf.image(image_path, 0, 0, 210, 297)  # Place on A4-sized paper\n",
        "\n",
        "        pdf.output(chunk_filename)\n",
        "        print(f\"[+] PDF chunk created: {chunk_filename}\")\n",
        "\n",
        "    # Combine PDF chunks into a single file\n",
        "    merger = PdfMerger()\n",
        "    for pdf_file in pdf_files:\n",
        "        merger.append(pdf_file)\n",
        "\n",
        "    merger.write(pdf_filename)\n",
        "    merger.close()\n",
        "\n",
        "    # Cleanup temporary PDF chunks\n",
        "    for pdf_file in pdf_files:\n",
        "        os.remove(pdf_file)\n",
        "\n",
        "    print(f\"[+] Final PDF created: {pdf_filename}\")\n",
        "\n",
        "# Start downloading images\n",
        "download_images_concurrently(start, end)\n",
        "\n",
        "# Create PDF from downloaded images\n",
        "images_to_pdf(folderName, pdfName)\n"
      ],
      "metadata": {
        "id": "u_7y1QtQc0Mc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}